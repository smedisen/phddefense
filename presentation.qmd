---
title: "Knowledge Discovery for<br> Interactive Decision Support and<br> Knowledge-Driven Optimization"
author: "Henrik Smedberg"
subtitle: "***Main Supervisor:***<br>**Sunith Bandaru**, Associate Professor, University of Skövde<br><br>***Co-supervisors:***<br>**Amos Ng**, Professor, University of Skövde<br>**Maria Riveiro**, Professor, Jönköping University"
footer: "<span style='padding-left: 300px;'>his.se/smeh</span> <span style='padding-left: 150px; padding-right: 15px;'>PhD Defense Presentation</span> | <span style='padding-left: 15px;'>Henrik Smedberg</span>"
bibliography: references.bib
# title-slide-attributes:
#     data-background-image: title-background.png
#     data-background-size: contain
format: 
  revealjs:
    css: style.css
    # background-image: slide-background.png 
    # background-size: contain
    date: 09/27/2023
    date-format: "DD MMM, YYYY"
    slide-number: true
    chalkboard: true
    fig-format: svg
    margin: 0.15
    width: 1280
    height: 720
    auto-play-media: true
---

# Introduction
::: {.ignore style="display: none"}

@trendmining2
@infspaper
@ejorpaper
@mimerpaper
@offlinekdo
@fpmoperator
@infskdo

<style>
.reveal .footer {
  height: 5%;
  bottom: -2px;
  background-color: #c0348b;
}

.reveal .footer p{
  font-family: Arial;
  text-align: left;
  padding-left: 3vw;
  color: #eeeeee;
  font-size: 1em;
  font-size: 1vw;
  position: absolute;
  top: 50%;
  left: 15%;
  padding: 0;
  margin: 0;
  transform-origin: center center;
  transform: translateX(calc((100% / 2) * (-1))) translateY(calc((100% / 2) * (-1)));
}

.reveal .slide-number {
  font-size: 1em;
  font-size: 1vw;
  padding: 0;
  margin: 0;
  bottom: 1.5%;
  transform-origin: center center;
  color: #eeeeee;
}

.reveal table {
  border-top: 2px solid black;
  border-bottom: 2px solid black;
}

.reveal .header {
  border-bottom: 1px solid black;
}

.reveal table td {
 border-bottom: 0px;
}
</style>

:::


## Introduction

::: {.slidecontent style="position: absolute; top: 120px;"}
:::: {.columns}

::: {.column width="50%"}
Optimization Example 

* Design *the best* car
  * Objectives
    1. High horsepower
    1. Low fuel consumption 

  * Variables
    1. Materials
    1. Engine size
    1. Dimensions
    1. ...
:::

::: {.column width="50%"}
<!-- LTeX: enabled=false -->
```{python}
# type: ignore
import numpy as np
import matplotlib.pyplot as plt
import math
import matplotlib as mpl
# import pandas as pd
from matplotlib import rc
import random
# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})
# rc('font',**{'family':'serif','serif':['Times New Roman']})
mpl.rcParams['pdf.fonttype'] = 42
mpl.rcParams['ps.fonttype'] = 42

mpl.rcParams['axes.linewidth'] = 0.5
mpl.rcParams["mathtext.fontset"] = "cm"
mpl.rcParams.update({'font.size': 18})
mpl.rcParams["legend.handletextpad"] = 0.0
rc('legend', fontsize=14)

rc('text', usetex=True)

def add_arrows(ax = None):
  if ax == None:
    ax = plt.gca() 

  ax.spines["left"].set_position(("data", -0.1))
  ax.spines["bottom"].set_position(("data", -0.1))

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.plot(1, -0.1, ">k", transform=ax.get_yaxis_transform(), clip_on=False)
  ax.plot(-0.1, 1, "^k", transform=ax.get_xaxis_transform(), clip_on=False)

xs = []
ys = []
xs2 = []
ys2 = []

for x in range(5):
    x = x/5
    angle = x*(math.pi/2)
    xs.append(math.cos(angle)*0.6 );
    ys.append(math.sin(angle)*0.6 );

random.seed(3)
for x in range(10):
  angle = random.random() * (math.pi/2)
  xs2.append(math.cos(angle) * (random.random() * 0.4) + 0.1);
  ys2.append(math.sin(angle) * (random.random() * 0.4) + 0.1);

ys = -np.array(ys)
ysmin = ys.min()
ys -= ysmin + 0.01

xs = np.array(xs) - 0.15

ys2 = -np.array(ys2)
ys2 -= ysmin + 0.01

xs2 = np.array(xs2) - 0.15

plt.scatter(xs, ys, c="#c0348b")
plt.scatter(xs2, ys2, facecolors='none', edgecolors="#c0348b")

plt.xlabel(r'Horsepower')
plt.ylabel(r'Fuel consumption')

plt.legend(['Optimal', "Dominated"], loc="lower right", bbox_to_anchor = [1, 0.07])

plt.xticks([])
plt.yticks([])
add_arrows()

plt.axis('square')
plt.show()
```
<!-- LTeX: enabled=true -->
:::
::::
:::

::: {.notes}

1. Set the stage
1. Optimize to find the best possible car
1. What is *the best*? **Objectives** point on the figure
1. We can only change **input variables**

:::

## Introduction

::: {.slidecontent style="position: absolute; top: 120px;"}
:::: {.columns}

::: {.column width="50%"}
* *Trade-offs* in the objective space

::: {style="padding-top: 0.75vw"}
* Decision Making 
  * Choose the final car to implement in practice

::: {.fragment .fade-in}
<ul><li style="list-style-type: none">
  <ul>
    <li> How are the cars different? </li>
    <li> How are the cars similar? </li>
    <li> What is *special* about the optimal cars? </li>
  </ul>
</li></ul>
:::
:::

::: {.fragment .fade-in}
* Knowledge Discovery
  * What can we *learn* from the optimization?

:::
:::

::: {.column width="50%"}

```{python}
# type: ignore
import numpy as np
import matplotlib.pyplot as plt
import math
import random

xs = []
ys = []
xs2 = []
ys2 = []

for x in range(5):
    x = x/5
    angle = x*(math.pi/2)
    xs.append(math.cos(angle)*0.6 );
    ys.append(math.sin(angle)*0.6 );

random.seed(3)
for x in range(10):
  angle = random.random() * (math.pi/2)
  xs2.append(math.cos(angle) * (random.random() * 0.4) + 0.1);
  ys2.append(math.sin(angle) * (random.random() * 0.4) + 0.1);

ys = -np.array(ys)
ysmin = ys.min()
ys -= ysmin + 0.01

xs = np.array(xs) - 0.15

ys2 = -np.array(ys2)
ys2 -= ysmin + 0.01

xs2 = np.array(xs2) - 0.15

plt.scatter(xs, ys, c="#c0348b")
plt.scatter(xs2, ys2, facecolors='none', edgecolors="#c0348b")

plt.xlabel(r'Horsepower')
plt.ylabel(r'Fuel consumption')

plt.legend(['Optimal', "Dominated"], loc="lower right", bbox_to_anchor = [1, 0.07])

plt.xticks([])
plt.yticks([])
add_arrows()

plt.axis('square')
plt.show()
```
:::
::::
:::


::: {.notes}

1. Considering several objectives at the same time gives Trade-offs
2. We need to analyze and decide which solution to implement
3. But we might also consider...
4. Knowledge discovery
5. This presentation will talk about these terms

:::

## Introduction

::: {.slidecontent .v-center}

:::: {.columns}
::: {.column}
* Multi-Objective Optimization
  * Find *optimal* solutions

::: {}

:::

* Decision Making
  * *Analysis* to find *preferred solutions*
  * Determine *final solution* to implement in practice

::: {.fragment .fade-in}
<ul><li style="list-style-type: none">
  <ul>
    <li> Update *problem formulation* </li>
    <li> *Learn* for *future* optimization cases </li>
  </ul>
</li></ul>
:::
:::

::: {.column .v-center .fragment .fade-in}
* Knowledge Discovery
  * Data mining and machine learning
  * Find relationships, patterns, rules, ... 
  * Describe *good* solutions in terms of the *decision space*  
    &nbsp;
  <li class="important-li"> Inform *Decision Maker* (DM) to make better decisions</li>

:::

::::
:::

::: {.notes}
1. Optimization for finding solutions
2. Decision making for analysis and choosing a final solution
3. But that's not all...
4. You might want to learn the capabilities of the formulation
5. Learn to improve the next-coming optimization
1. Knowledge Discovery ...
1. You know your preferred solutions in the objective space, what makes them good?
1. A more informed DM is able to make more informed, *better*, decisions

:::

<!-- ## Introduction -->
<!---->
<!-- ::: {.slidecontent .v-center} -->
<!---->
<!-- * Knowledge Discovery -->
<!--   * Data mining and machine learning -->
<!--   * Find relationships, patterns, rules, ...  -->
<!--   * Describe *good* solutions in terms of the *decision space*   -->
<!--     &nbsp; -->
<!--   <li class="important-li"> Inform *Decision Maker* (DM) to make better decisions</li> -->
<!---->
<!-- ::: -->
<!---->
<!-- ::: {.notes} -->
<!-- Knowledge Discovery ... -->
<!---->
<!-- You know your preferred solutions in the objective space, what makes them good? -->
<!---->
<!-- A more informed DM is able to make more informed, *better*, decisions -->
<!-- ::: -->

## Introduction

::: {.slidecontent .v-center}
:::: {.columns}

::: {.column width="50%"}
```{python}
# type: ignore
import numpy as np
import matplotlib.pyplot as plt
import math
import random


xs = []
ys = []
xs2 = []
ys2 = []

random.seed(13)
# for x in range(5):
#   x, y = 1,1
#   while x > 0.25 or y > 0.25:
#     x = random.random()
#     y = random.random()
#   xs.append(x);
#   ys.append(y);

xs = [0.0, 0.2, 0.2, 0.0, 0.1]
ys = [0.0, 0.0, 0.2, 0.2, 0.1]

for x in range(10):
  x, y = 0,0
  while x < 0.25 and y < 0.25:
    x = random.random()
    y = random.random()
  xs2.append(x);
  ys2.append(y);

ys = np.array(ys)
xs = np.array(xs)

ys2 = np.array(ys2)
xs2 = np.array(xs2)

plt.scatter(xs, ys, c="#c0348b")
plt.scatter(xs2, ys2, facecolors='none', edgecolors="#c0348b")

plt.xlabel(r'$x_1$')
plt.ylabel(r'$x_2$')


plt.xticks([0, 0.25, 0.5,0.75,1])
plt.xlim([0,1])
plt.yticks([0, 0.25, 0.5,0.75,1])
plt.ylim([0,1])
plt.title("Decision Space")
# plt.yticks([])
add_arrows()

plt.axis('square')
plt.show()
```
:::

::: {.column width="50%"}

```{python}
# type: ignore
import numpy as np
import matplotlib.pyplot as plt
import math
import random

xs = []
ys = []
xs2 = []
ys2 = []

for x in range(5):
    x = x/5
    angle = x*(math.pi/2)
    xs.append(math.cos(angle)*0.6 );
    ys.append(math.sin(angle)*0.6 );

random.seed(3)
for x in range(10):
  angle = random.random() * (math.pi/2)
  xs2.append(math.cos(angle) * (random.random() * 0.4) + 0.1);
  ys2.append(math.sin(angle) * (random.random() * 0.4) + 0.1);

ys = -np.array(ys)
ysmin = ys.min()
ys -= ysmin + 0.01

xs = np.array(xs) - 0.15

ys2 = -np.array(ys2)
ys2 -= ysmin + 0.01

xs2 = np.array(xs2) - 0.15

plt.scatter(xs, ys, c="#c0348b")
plt.scatter(xs2, ys2, facecolors='none', edgecolors="#c0348b")

plt.xlabel(r'Horsepower')
plt.ylabel(r'Fuel consumption')

plt.legend(['Optimal', "Dominated"], loc="lower right", bbox_to_anchor = [1, 0.07])

plt.xticks([])
plt.yticks([])
add_arrows()

plt.title("Objective Space")
plt.axis('square')
plt.show()
```
:::
::::
:::

<!-- lTex: enabled=false -->

<canvas id="canvas" style="width: 1000px; height: 500px; position: absolute;" class="fragment fade-in visible current-fragment" data-fragment-index="0">
</canvas>
<script>
function getBezierXY(t, sx, sy, cp1x, cp1y, cp2x, cp2y, ex, ey) {
  return {
    x: Math.pow(1-t,3) * sx + 3 * t * Math.pow(1 - t, 2) * cp1x + 3 * t * t * (1 - t) * cp2x + t * t * t * ex,
    y: Math.pow(1-t,3) * sy + 3 * t * Math.pow(1 - t, 2) * cp1y + 3 * t * t * (1 - t) * cp2y + t * t * t * ey
  };
}

function getBezierAngle(t, sx, sy, cp1x, cp1y, cp2x, cp2y, ex, ey) {
  var dx = Math.pow(1-t, 2)*(cp1x-sx) + 2*t*(1-t)*(cp2x-cp1x) + t * t * (ex - cp2x);
  var dy = Math.pow(1-t, 2)*(cp1y-sy) + 2*t*(1-t)*(cp2y-cp1y) + t * t * (ey - cp2y);
  return -Math.atan2(dx, dy) + 0.5*Math.PI;
}

function drawLine(start_point, end_point, ctrl_point1, ctrl_point2, draw_dots = false, draw_arrow = true) {
    line_color = "#c0348b";

    ctx.beginPath();
    ctx.moveTo(start_point[0], start_point[1]);
    ctx.bezierCurveTo(ctrl_point1[0], ctrl_point1[1], ctrl_point2[0], ctrl_point2[1], end_point[0], end_point[1]);
    ctx.strokeStyle = line_color;
    ctx.lineWidth = 5;
    ctx.stroke();
    
    if (draw_dots) {
        ctx.beginPath();
        ctx.fillStyle = '#88f';
        // start point
        ctx.arc(start_point[0], start_point[1], 5, 0, Math.PI * 2);
        // end point
        ctx.arc(end_point[0], end_point[1], 5, 0, Math.PI * 2);
        ctx.fill();

        ctx.beginPath();
        ctx.fillStyle = '#f88';
        // control point one
        ctx.arc(ctrl_point1[0], ctrl_point1[1], 5, 0, Math.PI * 2);
        // control point two
        ctx.arc(ctrl_point2[0], ctrl_point2[1], 5, 0, Math.PI * 2);
        ctx.fill();
    }

    if (draw_arrow) {
        var coord = getBezierXY(0.999, start_point[0], start_point[1], ctrl_point1[0], ctrl_point1[1], ctrl_point2[0], ctrl_point2[1], end_point[0], end_point[1]);
        var angle = getBezierAngle(0.999, start_point[0], start_point[1], ctrl_point1[0], ctrl_point1[1], ctrl_point2[0], ctrl_point2[1], end_point[0], end_point[1]);

        ctx.save();
        ctx.beginPath();
        ctx.translate(coord.x, coord.y);
        ctx.rotate(angle);
        ctx.moveTo(-10, -10);
        ctx.lineTo(10, 0);
        ctx.lineTo(-10, 10);
        ctx.lineTo(-10, -10);
        ctx.fillStyle = line_color;
        ctx.fill();
        ctx.restore();
    }
}

var canvas = document.getElementById('canvas');
var ctx = canvas.getContext('2d');

var displayWidth = 1000;
var displayHeight = 500;
var scale = 2;
canvas.style.width = displayWidth + 'px';
canvas.style.height = displayHeight + 'px';
canvas.width = displayWidth * scale;
canvas.height = displayHeight * scale;


start_point = [438, 502];
end_point = [1635, 496];
ctrl_point1 = [800, 60];
ctrl_point2 = [1000, 120];
drawLine(start_point, end_point, ctrl_point1, ctrl_point2);

start_point = [375, 795];
end_point = [1685, 695];
ctrl_point1 = [800, 1100];
ctrl_point2 = [1500, 920];
drawLine(start_point, end_point, ctrl_point1, ctrl_point2);
</script>

<!-- lTex: enabled=true -->

::: {.notes}

* If we consider the same solutions as before
* we also have the decision space for the solutions
* this solution here, might be here...
* Solutions down here might be optimal
* We can see a pattern for the optimal solutions

:::

## Introduction 


::: {.slidecontent .v-center}
::::{.columns}
::: {.column width="35%"}
* Optimal solutions
  * $x_1 < 0.25$
  * $x_2 < 0.25$

::: {style="padding-top: 70px; width: 200%; display: block;"}
* *Decision rules* as knowledge
:::
:::

::: {.column width="50%"}
::::{.columns}
::: {.column width="50%" .threequarter style="position: absolute; left: 400px; top: -72px;"}
```{python}
# type: ignore
import numpy as np
import matplotlib.pyplot as plt
import math
import random


xs = []
ys = []
xs2 = []
ys2 = []

random.seed(13)
# for x in range(5):
#   x, y = 1,1
#   while x > 0.25 or y > 0.25:
#     x = random.random()
#     y = random.random()
#   xs.append(x);
#   ys.append(y);

xs = [0.0, 0.2, 0.2, 0.0, 0.1]
ys = [0.0, 0.0, 0.2, 0.2, 0.1]

for x in range(10):
  x, y = 0,0
  while x < 0.25 and y < 0.25:
    x = random.random()
    y = random.random()
  xs2.append(x);
  ys2.append(y);

ys = np.array(ys)
xs = np.array(xs)

ys2 = np.array(ys2)
xs2 = np.array(xs2)

plt.scatter(xs, ys, c="#c0348b")
plt.scatter(xs2, ys2, facecolors='none', edgecolors="#c0348b")

plt.xlabel(r'$x_1$')
plt.ylabel(r'$x_2$')


plt.xticks([0, 0.25, 0.5,0.75,1])
plt.xlim([0,1])
plt.yticks([0, 0.25, 0.5,0.75,1])
plt.ylim([0,1])
plt.title("Decision Space")
# plt.yticks([])
add_arrows()

plt.axis('square')
plt.show()
```
:::

::: {.column width="50%" .threequarter style="position: absolute; left: 800px; top: -72px;"}
```{python}
# type: ignore
import numpy as np
import matplotlib.pyplot as plt
import math
import random

xs = []
ys = []
xs2 = []
ys2 = []

for x in range(5):
    x = x/5
    angle = x*(math.pi/2)
    xs.append(math.cos(angle)*0.6 );
    ys.append(math.sin(angle)*0.6 );

random.seed(3)
for x in range(10):
  angle = random.random() * (math.pi/2)
  xs2.append(math.cos(angle) * (random.random() * 0.4) + 0.1);
  ys2.append(math.sin(angle) * (random.random() * 0.4) + 0.1);

ys = -np.array(ys)
ysmin = ys.min()
ys -= ysmin + 0.01

xs = np.array(xs) - 0.15

ys2 = -np.array(ys2)
ys2 -= ysmin + 0.01

xs2 = np.array(xs2) - 0.15

plt.scatter(xs, ys, c="#c0348b")
plt.scatter(xs2, ys2, facecolors='none', edgecolors="#c0348b")

plt.xlabel(r'Horsepower')
plt.ylabel(r'Fuel consumption')

plt.legend(['Optimal', "Dominated"], loc="lower right", bbox_to_anchor = [1, 0.07])

plt.xticks([])
plt.yticks([])
add_arrows()

plt.title("Objective Space")
plt.axis('square')
plt.show()
```
:::
::::
:::
::::
:::

<canvas id="canvas2" width="650" height="347" style="top: 150px; left: 544px; position: absolute" class="fragment fade-in visible current-fragment" data-fragment-index="0">
</canvas>

<script type="text/javascript">
    var canvas = document.getElementById('canvas2');
    var ctx = canvas.getContext('2d');

    var displayWidth = 650;
    var displayHeight = 347;
    var scale = 2;
    canvas.style.width = displayWidth + 'px';
    canvas.style.height = displayHeight + 'px';
    canvas.width = displayWidth * scale;
    canvas.height = displayHeight * scale;

    ctx.beginPath();
    ctx.moveTo(0, 180*2 + 158);
    ctx.lineTo(90*2, 180*2 + 158);
    ctx.lineTo(90*2, 270*2 + 158);
    ctx.strokeStyle = '#c0348b';
    ctx.lineWidth = 3;
    ctx.stroke();

    ctx.drawImage(canvas, 0, 0);

    start_point = [850, 490+160];
    end_point = [1205, 20+160];
    ctrl_point1 = [(start_point[0]+end_point[0])/2, start_point[1]];
    ctrl_point2 = [end_point[0], (start_point[1]+end_point[1])/2];
    drawLine(start_point, end_point, ctrl_point1, ctrl_point2, false, false);

    sp1 = start_point;
    ep1 = end_point;

    start_point = [850, 490-70 + 160];
    end_point = [1193-50, 20+160];
    ctrl_point1 = [(start_point[0]+end_point[0])/2, start_point[1]];
    ctrl_point2 = [end_point[0], (start_point[1]+end_point[1])/2];
    drawLine(start_point, end_point, ctrl_point1, ctrl_point2, false, false);

    sp2 = start_point;
    ep2 = end_point;

    start_point = sp1;
    end_point = sp2;
    ctrl_point1 = [(start_point[0]+end_point[0])/2-50, start_point[1]];
    ctrl_point2 = [(start_point[0]+end_point[0])/2-50, end_point[1]];
    drawLine(start_point, end_point, ctrl_point1, ctrl_point2, false, false);

    start_point = ep1;
    end_point = ep2;
    ctrl_point1 = [start_point[0], (start_point[1]+end_point[1])/2-50];
    ctrl_point2 = [end_point[0], (start_point[1]+end_point[1])/2-50];
    drawLine(start_point, end_point, ctrl_point1, ctrl_point2, false, false);
</script>


::: {.notes}
1. Optimal solutions can be described with...
1. **Click**
2. This is the kind of design rule knowledge we want to find in this thesis
:::

## Introduction

::: {.slidecontent .v-center}

::: {}
Knowledge-Driven Optimization (*KDO*) [@parta]

* Use knowledge in *optimization algorithms*
* Faster convergence on better solutions
:::

::: {style="height: 2vw;"}
:::

:::: {.columns}
::: {.column .fragment .fade-in}
* *Offline KDO*
  * Incorporate knowledge from *previous* optimizations, in *future* optimizations
  * Involves the DM
:::
::: {.column .fragment .fade-in}
* *Online KDO*
  * *Discover* and *exploit* knowledge,  
    as part of the algorithm
  * Also generates knowledge for the DM
:::
::::
:::

::: {.notes}

But not only for decision making  
Also for KDO  
Can be realized in two ways...

1. Offline
1. Online

:::

## Introduction

::: {.slidecontent .v-center}
<span style="font-size: 125%;">Motivation</span>

* Knowledge Discovery
  * Benefits to *decision support* by offering *insights* into the optimization process, and results

::: {style="height: 20px;"}
:::

* Knowledge-Driven Optimization
  * Benefits to *optimization* by utilizing design rules for *improved convergence behaviors*
:::

# Background

## Background

::: {.slidecontent}
:::: {.columns}
::: {.column width="60%"}
* <span style="width:200%; display:block;">Multi-Objective Evolutionary Algorithms (*MOEAs*)</span>
  * Inspired by behaviors found in nature
  * Evolve a population of solutions over generations
  * Balance *convergence* and *diversity*  
    &nbsp;
  <li class="minus-li"> Generate many sub-optimal solutions</li>
:::
::: {.column width="40%"}
::: {style="font-size: 50%; padding-top: 75px;"}
::: {style="padding-left: 100px;"}
MOOP formulation:
:::
<!-- lTex: enabled=false -->
\begin{align*}
\text{Minimize} &\qquad \textbf{F}(\textbf{x}) = [f_1(\textbf{x}), f_2(\textbf{x}), \dots, f_M(\textbf{x})]^T \\
\text{Subject to} &\qquad g_j(\textbf{x}) \geq 0 \ \forall j = 1, 2, \dots, J \\
&\qquad h_k(\textbf{x}) = 0 \ \forall k = 1, 2, \dots, K \\
&\qquad \textbf{x}^{(L)} \leq \textbf{x} \leq \textbf{x}^{(U)}
\end{align*}
<!-- lTex: enabled=true -->

:::
:::
::::

:::: {.columns}
::: {.column style="padding-top: 50px; width:40%;"}
* NSGA-II [@deb2002fast]
  * Non-Dominated Sorting
  * Crowding distance
:::

::: {.column width="60%"}
::: {layout="[1,1,1]" style="padding-left:60px; padding-top:40px;"}
```{python}
# type: ignore

import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
from matplotlib import rc
# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})
# rc('font',**{'family':'serif','serif':['Times New Roman']})
mpl.rcParams['pdf.fonttype'] = 42
mpl.rcParams['ps.fonttype'] = 42

mpl.rcParams['axes.linewidth'] = 0.5
mpl.rcParams["mathtext.fontset"] = "cm"
mpl.rcParams.update({'font.size': 14})
mpl.rcParams["legend.handletextpad"] = 0.0
rc('legend', fontsize=10)

rc('text', usetex=True)

def add_arrows(ax = None):
  if ax == None:
    ax = plt.gca() 

  ax.spines["left"].set_position(("data", -0.1))
  ax.spines["bottom"].set_position(("data", -0.1))

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.plot(1, -0.1, ">k", transform=ax.get_yaxis_transform(), clip_on=False)
  ax.plot(-0.1, 1, "^k", transform=ax.get_xaxis_transform(), clip_on=False)

import pandas as pd

df = pd.read_csv("data/nds_example.csv", sep=';')
df['x'] += 0.5
df['y'] += 0.45

plt.figure(figsize=(5.5/2,5.5/2))

plt.scatter(df['x'], df['y'], marker='o', facecolors='none', edgecolors='black', linewidths=0.5)

plt.xlabel(r'$f_1({\bf x })$')
plt.ylabel(r'$f_2({\bf x })$')

plt.xticks([])
plt.yticks([])
add_arrows()

plt.scatter([1.1], [1.1], facecolors='none', edgecolors='none')

plt.axis('square')
plt.show()
```
```{python}

plt.figure(figsize=(5.5/2,5.5/2))

ax = plt.gca()

ax.text(0,0.5, "$Non$-$dominated\ sorting$\n")
ax.annotate("", xy=(0.96, 0.53), xycoords='data', xytext=(0., 0.53), textcoords='data', 
    arrowprops=dict(arrowstyle="->", connectionstyle="arc3"),)
            
plt.axis('off')

# add_arrows()
# plt.xlabel(r'$f_1({\bf x })$')
# plt.ylabel(r'$f_2({\bf x })$')
plt.xticks([])
plt.yticks([])

# plt.axis('square')
plt.show()
```

```{python}

plt.figure(figsize=(5.5/2,5.5/2))

colors = ['#e34a33','#fdbb84','#fee8c8']
cs = [colors[x] for x in df['nds_rank'].values]
for nds in [0,1,2]:
  ddf = df[df['nds_rank'] == nds]
  plt.scatter(ddf['x'], ddf['y'], marker='o', edgecolors='black', c=colors[nds], linewidths=0.5)
  
plt.xlabel(r'$f_1({\bf x })$')
plt.ylabel(r'$f_2({\bf x })$')

# plt.legend(['$Rank\ 0$', '$Rank\ 1$', '$Rank\ 2$'], loc="lower left", bbox_to_anchor=(0.05, 0.05))
plt.legend(['$Rank\ 0$', '$Rank\ 1$', '$Rank\ 2$'], loc="upper right")

add_arrows()
plt.xlabel(r'$f_1({\bf x })$')
plt.ylabel(r'$f_2({\bf x })$')
plt.xticks([])
plt.yticks([])

plt.scatter([1.1], [1.1], facecolors='none', edgecolors='none')

plt.axis('square')
plt.show()
```

:::
:::
::::

::: {style="position: relative; right: -800px; top: -320px; font-size: 50%;"}
Minimize $f_1(\bf x)$ and $f_2(\bf x)$
:::

:::

::: {.notes}
* We use MOEAs to solve MOOPs...
* ...
* Sub-optimal solutions are wasteful  
* we can use knowledge to reduce number of solutions needed to converge
* One of the very common MOEAs is NSGA-II
:::

## Background

::: {.slidecontent .v-center}


Multi-Criteria Decision Making

* How to involve the DM? [@miettinen1999nonlinear]
  1. *no preference* --- No preferences are available or used
  1. *a priori* --- Express preferences before the optimization 
  1. *a posteriori* --- Express preferences after optimization 
  1. *interactive* --- Update preferences during optimization

::: {layout="[[1, 1],[1, 1]]" style="scale: 100%; position: absolute; top: -100px; right: 0px;"}
```{python}
# type: ignore

import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
from matplotlib import rc
import pandas as pd

df = pd.read_csv("data/fewpoints.csv", sep=';')
df = (df - df.min()) / (df.max() - df.min())

for i, val in enumerate([0.25,0.15,0.,0.15,0.25]):
    df['x'][i] += val
    df['y'][i] += val

df['x'] -= 0.15
df['y'] -= 0.15

plt.figure(figsize=(5.5/2,5.5/2))

plt.scatter(df['x'], df['y'], marker='o', facecolors='#e34a33', edgecolors='black', linewidths=0.5)

plt.xlabel(r'$f_1({\bf x })$')
plt.ylabel(r'$f_2({\bf x })$')

plt.xticks([])
plt.yticks([])
add_arrows()

plt.scatter([1.9], [1.9], facecolors='none', edgecolors='none', label="1.")
plt.gca().legend(loc="upper right", fancybox=False, frameon=False, fontsize=15)

plt.axis('square')
plt.show()
```

```{python}
# type: ignore

import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
from matplotlib import rc
import pandas as pd

df = pd.read_csv("data/fewpoints.csv", sep=';')
df = (df - df.min()) / (df.max() - df.min())

df['x'] *= 0.25
df['y'] *= 0.5
df['y'] += 0.25
df['x'] += 0.1

plt.figure(figsize=(5.5/2,5.5/2))

plt.scatter(df['x'], df['y'], marker='o', facecolors='#e34a33', edgecolors='black', linewidths=0.5)
plt.scatter([0.1], [0.35], facecolors='#47ff7e', edgecolors='none', marker='*', s=150)

plt.xlabel(r'$f_1({\bf x })$')
plt.ylabel(r'$f_2({\bf x })$')

plt.xticks([])
plt.yticks([])
add_arrows()

plt.scatter([1.3], [1.3], facecolors='none', edgecolors='none', label="2.")

plt.gca().legend(loc="upper right", fancybox=False, frameon=False, fontsize=15)

plt.axis('square')
plt.show()
```

```{python}
# type: ignore

import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
from matplotlib import rc
import pandas as pd
from matplotlib.patches import Ellipse

df = pd.read_csv("data/fewpoints.csv", sep=';')
df = (df - df.min()) / (df.max() - df.min())

plt.figure(figsize=(5.5/2,5.5/2))

plt.scatter(df['x'], df['y'], marker='o', facecolors='#e34a33', edgecolors='black', linewidths=0.5)

ax = plt.gca()
ellipse = Ellipse(xy=(0.3, 0.55), width=0.56, height=0.32, edgecolor='#47ff7e', fc='None', lw=2, angle=-45)
ax.add_patch(ellipse)

plt.xlabel(r'$f_1({\bf x })$')
plt.ylabel(r'$f_2({\bf x })$')

plt.xticks([])
plt.yticks([])
add_arrows()

plt.scatter([1.3], [1.3], facecolors='none', edgecolors='none', label="3.")
plt.gca().legend(loc="upper right", fancybox=False, frameon=False, fontsize=15)

plt.axis('square')
plt.show()
```
```{python}
# type: ignore

import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
from matplotlib import rc
import pandas as pd

df = pd.read_csv("data/fewpoints.csv", sep=';')
df = (df - df.min()) / (df.max() - df.min())

df['y'] *= 0.25
df['x'] *= 0.5
df['x'] += 0.25
df['y'] += 0.1

plt.figure(figsize=(5.5/2,5.5/2))

plt.scatter(df['x'], df['y'], marker='o', facecolors='#e34a33', edgecolors='black', linewidths=0.5)
plt.plot([1.0, 0.4, 0.7, 0.5], [1.0,0.95, 0.5, 0.3], c='#47ff7e')
plt.scatter([0.40, 0.7, 0.5], [0.95, 0.5, 0.3], facecolors='#47ff7e', edgecolors='none', marker='*', s=150)

plt.xlabel(r'$f_1({\bf x })$')
plt.ylabel(r'$f_2({\bf x })$')

plt.xticks([])
plt.yticks([])
add_arrows()

plt.scatter([1.3], [1.3], facecolors='none', edgecolors='none', label="4.")
plt.gca().legend(loc="upper right", fancybox=False, frameon=False, fontsize=15)

plt.axis('square')
plt.show()
```
:::

:::

::: {.notes}
1. Optimization is one side of the coin, we also have decision making
1. incorporating DM's preferences and make decisions 
1. 4 classes of methods for incorporating DM's preferences
1. Not directly, but finding interesting solutions like knee-points
1. ...
:::

## Background

::: {.slidecontent}

:::: {.columns}
:::{.column width="35%"}
* Visualize Solutions
  * Scatter Plot
  * Parallel Coordinate Plot
  * RadViz
  * Heatmap
  * Spider chart
  * ...
:::
::: {.column width="65%"}
```{python}
#| layout-ncol: 3
#| layout-valign: bottom

# type: ignore 
import pandas as pd
import math

def set_3d_scatter(ax):
  for axis in [ax.xaxis,ax.yaxis,ax.zaxis]:
    axis.pane.fill = False
    axis.pane.set_edgecolor('#000')
    axis.pane.set_alpha(1)
    axis.pane.set_linewidth(0.5)
    axis._axinfo["grid"]['linestyle'] = ":"
    axis._axinfo["grid"]['linewidth'] = 0.25
  
  ax.spines["top"].set_linewidth(0.5)
  ax.spines["right"].set_linewidth(0.5)
  ax.spines["left"].set_linewidth(0.5)
  ax.spines["bottom"].set_linewidth(0.5)

  ax.set_proj_type('ortho')
  ax.view_init(30, 45)

df = pd.read_csv("data/dtlz1space.csv", sep=' ')
colors = []
for index, d in df.iterrows():
  colors.append((d['r'], d['g'], d['b']))


fig = plt.figure(figsize=(5.5/2,5.5/2))
ax = fig.add_subplot(projection='3d')

set_3d_scatter(ax)
ax.scatter(df['f1'], df['f2'], df['f3'], c=colors, linewidths=0.5, alpha=0.75)

ax.set_xlabel('$f_1$')
ax.set_ylabel('$f_2$')
ax.set_zlabel('$f_3$')

ax.set_box_aspect((1,1,1))
plt.title('3D Scatter Plot', y=-0.5)
plt.show()



plt.figure(figsize=(5.5/2,5.5/2))

labels =['f1','f2','f3']
print_labels =['$f_1$','$f_2$','$f_3$']
objectives = df[labels]

for index, d in df.iterrows():
  plt.plot(d[labels], c=colors[index], alpha=0.25)

plt.gca().set_xmargin(0)
plt.gca().set_ymargin(0)

plt.gca().set_xticklabels(print_labels)

plt.gca().set_yticks([0,1,2,3,4,5,6])
plt.gca().set_yticklabels(["$0$","","$2$","","$4$","","$6$"])

ax2 = plt.gca().twinx()

ax2.set_yticks([0,1,2,3,4,5,6])
ax2.set_yticklabels([])
ax2.spines['left'].set_position('center')
ax2.yaxis.set_ticks_position('both')

plt.gca().set_box_aspect(0.85)
plt.title('PCP', y=-0.5)
plt.show()



plt.figure(figsize=(6.5/2,6.5/2))


circle1 = plt.Circle((0, 0), 1.1, fill = False, color='k', linewidth=0.5)
plt.gca().add_patch(circle1)

xs = []; ys = []
txs = []; tys = []
for i in range(3):
  xs.append(math.cos(2*math.pi * i / 3) * 1.1)
  ys.append(math.sin(2*math.pi * i / 3) * 1.1)
  txs.append(math.cos(2*math.pi * i / 3) * 1.3)
  tys.append(math.sin(2*math.pi * i / 3) * 1.3)

plt.scatter(x=xs, y=ys, color='black')
plt.gca().annotate("$f_1$", xy=(0,0), xycoords='data', xytext=(txs[0], tys[0]), textcoords='data', ha='center', va='center', fontsize=12)
plt.gca().annotate("$f_2$", xy=(0,0), xycoords='data', xytext=(txs[1], tys[1]), textcoords='data', ha='center', va='center', fontsize=12)
plt.gca().annotate("$f_3$", xy=(0,0), xycoords='data', xytext=(txs[2], tys[2]), textcoords='data', ha='center', va='center', fontsize=12)

s = []; ls = []
for i in range(3):
    ls.append(2.0 * math.pi * (i / 3))
for t in ls:
  s.append([math.cos(t), math.sin(t)])

X = (df[labels] - df[labels].min()) / (df[labels].max() - df[labels].min())

xx = []; yy = []

for i, row in X.iterrows():
  SIG = 0
  for r in row:
    SIG += r
  sig = 0
  for i in range(len(row)):
    sig += row[i]*s[i][0]
  x = sig/SIG
  sig = 0
  for i in range(len(row)):
    sig += row[i]*s[i][1]
  y = sig/SIG
  xx.append(x)
  yy.append(y)

plt.scatter(xx,yy, color=colors, marker='o', linewidths=0.5, s=15, alpha=0.75)

a = 1.5

plt.xlim([-(a),a])
plt.ylim([-a,a])
plt.xticks([])
plt.yticks([])
ax = plt.gca()
ax.spines["top"].set_color("white")
ax.spines["right"].set_color("white")
ax.spines["left"].set_color("white")
ax.spines["bottom"].set_color("white")

ax.set_box_aspect(1)
# plt.axis('square')
plt.title('RadViz', y=-0.2)
plt.show()

```
:::
::::

:::: {.columns .fragment .fade-in}
::: {.column width="50%"}

* Challenges
    * How to visualize *many* objectives?
    * How to visualize *many* solutions?
    * How to visualize the *decision space*?
:::
::: {.column width="50%"}
<ul>
    <li style="list-style-type: none"> 
        <hidden>.</hidden>
    <ul>
    <li>How to visualize *preferences*?</li>
    <li>How to visualize *constraints*?</li>
    <li>How to visualize ***knowledge***?</li>
    </ul>
    </li>
</ul>
:::
::::

:::

::: {.notes}
Visualizations are a good tool for decision support    
Many ways to visualize the same solutions  
No method is better or worse for all solution sets

There are challenges to consider...
:::

## Background

::: {.slidecontent .v-center}
Knowledge Discovery in MOO

:::: {.columns}
::: {.column width="50%"}

* *Implicit* knowledge
  * Clustering [@jeong2008multidimensional]
  * Manifold learning 
    * Self-Organizing Maps [@kobayashi2016pareto]
    * t-SNE [@van2008visualizing]
  * ...

:::
::: {.column width="50%"}

* *Explicit* knowledge
  * Innovization [@deb2006innovization]
  * Decision rules [@sugimura2009kriging]
  * Variable trends [@bandaru2019trend]
  * ...

:::
::::

:::: {.columns .fragment .fade-in}
::: {.column width="50%"}

<ul>
  <li style="list-style-type: none">
    <Ul>
      <li>Does *not* have a clear, formal notation</li>
      <li>Can *not* be easily transferred</li>
      <li>Can *not* be directly applied without further interpretation</li>
      <!-- <li>Does *not* answer ***how?***</li> -->
    </ul>
  </li>
</ul>

:::
::: {.column width="50%"}

<ul>
  <li style="list-style-type: none">
    <Ul>
      <li>Has a clear, formal notation</li>
      <li>Can be transferred between processes</li>
      <li>Can be applied directly both *algorithmically* and for *decision support*</li>
    </ul>
  </li>
</ul>

:::
::::
:::

::: {.notes}
* Knowledge discovery has been applied in MOO before, and there are two types of knowledge
* Implicit knowledge, like clustering and manifold learning  
* and explicit knowledge, like Innovization, decision rules and variable trends

* I focused on explicit knowledge representations since they...
:::

# Research Approach

::: {.notes}
so what did I actually do?
:::

## Research Approach

::: {.slidecontent .v-center}
:::: {.columns}
::: {.column }
* Motivation
  * *Knowledge discovery* can benefit optimization and decision making

* Aim
  * Develop methods for meaningful *knowledge discovery* in MOO solutions
  * Appropriate for decision support and KDO
  * *Simple* and easy to understand knowledge 
:::
::: {.column style="padding-top: 10px;" .fragment .fade-in}

* Research Methodology
  * Design Science [@hevner2008design]
    * Designing *artifacts* for clearly defined *relevant problems*
    * Rigorous *testing and development* and a clear *contribution*
    * *Communicate results* with scientific communities and stakeholders  
    &nbsp;
  <li class="important-li"> Produce useful *artifacts*</li>
:::
::::
:::

::: {.notes}
The whole work is motivated by the belief that knowledge discovery can benefit optimization and decision making  

Design Science  
Produce useful artifacts by iteratively developing and testing them  
And they should have a contribution both to industrial needs and to the scientific literature

Algorithms, frameworks, instantiations, ...
:::

## Research Approach

::: {.slidecontent .rqs .v-center}

:::: {.columns}
::: {.column width="50%"}
::: {}
::: {style="position: absolute; "}
**RQ1**
:::
::: {style="position:absolute; padding-left: 70px; width: 55%"}
**Knowledge Discovery.** How can different *representations of knowledge* be extracted from the solutions of multi-objective optimization problems, while considering diverse multi-objective optimization scenarios?
:::
:::

::: {style="padding-top: 120px"}
::: {style="position: absolute; "}
**RQ2**
:::

::: {style="position:absolute; padding-left: 70px; width: 45%"}
**Decision Support.** How can different *representations of knowledge* be processed and presented to the decision maker in an *interactive* manner for better *decision support*?
:::
:::

::: {style="padding-top: 120px"}
::: {style="position: absolute; "}
**RQ3**
:::


::: {style="position:absolute; padding-left: 70px; "}
**Knowledge-Driven Optimization.** 
:::

::: {style="padding-top: 30px"}
::: {style="position: absolute; padding-left: 70px;"}
**a**
:::

::: {style="position:absolute; padding-left: 100px; width: 52%"}
**Offline KDO.** How can knowledge extracted from completed optimization runs be utilized *offline* in *future optimizations* of similar problem scenarios to focus the search towards preferred solutions?
:::
:::

::: {style="padding-top: 110px"}
::: {style="position: absolute; padding-left: 70px;"}
**b**
:::

::: {style="position:absolute; padding-left: 100px; width: 52%"}
**Online KDO.** How can knowledge extracted during an optimization run be utilized *online* to improve the *convergence behavior* of optimization algorithms?
:::
:::
:::
:::

::: {.column width="50%"}

![](figures/map.drawio.svg){style="padding-left: 40px; padding-top: 20px;"}
:::
:::
:::

::: {.notes} 
* RQ1 - finding appropriate and meaningful knowledge representations for MOO
* RQ2 - use knowledge discovery for decision support, how can it be used in a meaningful way?
* RQ3a - offline KDO, an important question is also to find appropriate use-cases for Offline KDO
* RQ3b - online KDO
:::

<!-- ## Research Approach

::: {.slidecontent .v-center}

Research Methodology

* Design Science [@hevner2008design]
  * Designing *artifacts* motivated by *business needs*
  * Rigorous *testing and development* and a clear *contribution*
  * *Communicate results* with scientific communities and stakeholders  
    &nbsp;

::: {.fragment .fade-in}
* Produce useful *artifacts*
:::
::: -->


<!-- ::: {.notes}
Design Science  
Produce useful artifacts by iteratively developing and testing them  
And they should have a contribution both to industrial needs and to the scientific literature

Algorithms, frameworks, instantiations, ...
::: -->

# Main Contributions

## Included Papers

::: {.slidecontent .papers}

:::: {.columns}
::: {.column width="50%"}
::: {style="width: 95%;"}
<!-- Publications with High Relevance -->

(@) **Smedberg, Henrik**, Bandaru, Sunith, Ng, Amos H.C., and Deb, Kalyanmoy (2020). “Trend Mining 2.0: Automating the Discovery of Variable Trends in the Objective Space”. In: *2020 IEEE Congress on Evolutionary Computation (CEC)*, pp. 1–8

(@) **Smedberg, Henrik** and Bandaru, Sunith (2020). “Finding Influential Variables in Multi-Objective optimization Problems”. In: *2020 IEEE Symposium Series on Computational Intelligence (SSCI)*, pp. 173–180 

(@) **Smedberg, Henrik** and Bandaru, Sunith (2023). “Interactive Knowledge Discovery and Knowledge Visualization for Decision Support in Multi-Objective Optimization”. In: *European Journal of Operational Research 306.3*, pp. 1311–1329

(@) **Smedberg, Henrik**, Bandaru, Sunith, Riveiro, Maria, and Ng, Amos H.C. (2023). “Mimer: A Web-Based Tool for Knowledge Discovery in Multi-Criteria Decision Support”. Submitted to: *IEEE Computational Intelligence Magazine*

(@) **Smedberg, Henrik**, Barrera-Diaz, Carlos Alberto, Nourmohammadi, Amir, Bandaru, Sunith, and Ng, Amos H. C. (2022). “Knowledge-Driven Multi-Objective Optimization for Reconfigurable Manufacturing Systems”. In: *Mathematical and Computational Applications 27.6*

(@) **Smedberg, Henrik** and Bandaru, Sunith (2022). “A Modular Knowledge-Driven Mutation Operator for Reference-Point Based Evolutionary Algorithms”. In: *2022 IEEE Congress on Evolutionary Computation (CEC)*, pp. 1–8

(@) **Smedberg, Henrik**, Bandaru, Sunith, and Ng, Amos H.C (2023). “Knowledge-Driven Multi-Objective Optimization Using Variable Importance”. *Completed Manuscript*
:::
:::

::: {.column width="50%"}
::: {style="font-size: 50%;" .v-center}
|  | RQ1<br><ssmall>Knowledge Discovery</ssmall> | RQ2<br><ssmall>Decision Support</ssmall> | RQ3a<br><ssmall>Offline KDO</ssmall> | RQ3b<br><ssmall>Online KDO</ssmall> |
| ---       | :---:    | :---:    | :---:    | :---:    |
| Paper I   | $\times$ |          |          |          |
| Paper II  | $\times$ |          |          |          |
| Paper III | $\times$ | $\times$ |          |          |
| Paper IV  |          | $\times$ |          |          |
| Paper V   |          | $\times$ | $\times$ |          |
| Paper VI  |          |          |          | $\times$ |
| Paper VII |          |          | $\times$ | $\times$ |

: {tbl-colwidths="[24, 33, 26, 20, 22]"}
:::
:::
::::
:::

::: {.notes} 
7 main papers to answer the RQs
:::

## RQ1: *Knowledge Discovery*

::: {.slidecontent}

::: {style="scale: 75%; padding-top: 10px; padding-bottom: 10px;"}
<italic>How can different *representations of knowledge* be extracted from the solutions of multi-objective optimization problems, while considering diverse multi-objective optimization scenarios?</italic>
:::

:::: {.columns}
::: {.column}
* Variable trends 
    * Finding interesting trends for variables along the objective space
    * *Trend Mining 2.0* <psmall>(Paper I)</psmall>

* Variable importance
    * Finding important variables for *diversity* and *convergence*
    * *InfS-P* <psmall>(Paper II)</psmall>
    * *InfS-R* <psmall>(Paper II)</psmall>
:::
::: {.column}
* Decision rules
    * Flexible Pattern Mining (*FPM*)  
    [@partb]
    * DM specifies *selected* and *unselected* sets of solutions in the *objective space*
    * FPM finds rules in the *decision space* that describes DM's *preferences*
    * Interactive *graph-based visualization* of FPM rules <psmall>(Paper III)</psmall>
:::
::::
:::

::: {.notes}
::: {style="font-size: 90%"}
This RQ is about different representations of knowledge, with at point that it should work in many different scenarios  
I primarily focused on three different knowledge representations for different aspects of MOO

Variable trends/trend mining is a way to find if individual variables follow an interesting trend in the decision space,  
as the solutions move in the objective space

Variable importance can be used to find if individual variables are more related to diversity or convergence

And a large part of my work focused on decision rules with FPM, which is a way to incorporate a DM's preferences,  
as selected and unselected sets of solutions in the objective space and find rules for these preferences in the decision space
:::
:::

## Variable Trends

::: {.slidecontent}


:::: {.columns}
::: {.column}
::: {.v-center style="width: 50%"}
* Trend Mining 2.0
  * <span style="width: 150%; display: block;">Find if *decision variables* follow a trend in the *objective space*</span>
  * Monotonic trends in the decision space
  * Find reference directions in the *objective space* that has the most interesting trends in the *decision space*  
    &nbsp;
  1. <lsmall>Finding an optimized reference direction</lsmall>
  2. <lsmall>Projection of solutions onto the reference direction</lsmall>
  3. <lsmall>Calculation of interestingness scores</lsmall>
  4. <lsmall>Boxplot visualization of interestingness scores</lsmall>

::: {style="height: 2vw;"}
:::

:::
:::

::: {.column style="margin-top: -30px"}
::: {layout="[[0.5,1,0.1],[1,1],[1,1]]" layout-valign="bottom"}

$\quad$

```{python}
# type: ignore

import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
from matplotlib import rc
#rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})
# rc('font',**{'family':'serif','serif':['Times New Roman']})
mpl.rcParams['pdf.fonttype'] = 42
mpl.rcParams['ps.fonttype'] = 42

mpl.rcParams['axes.linewidth'] = 0.5
mpl.rcParams["mathtext.fontset"] = "cm"
mpl.rcParams.update({'font.size': 10})
mpl.rcParams["legend.handletextpad"] = 0.0
rc('legend', fontsize=10)

rc('text', usetex=True)


data = [
	[22.4405, 20.3723, 20.1655, 19.3382, 21.6132, 22.0269, 21.6132, 20.9928, 19.7518, 19.9586],
	[21.6132, 21.6132, 21.1996, 21.6132, 21.6132, 21.4064, 21.6132, 18.9245, 20.9928, 21.4064],
	[72.0786, 72.2854, 72.6991, 71.6649, 71.8718, 72.0786, 72.0786, 71.8718, 72.2854, 72.2854],
	[23.4747, 23.6815, 23.8883, 23.8883, 23.6815, 23.8883, 23.6815, 22.4405, 23.6815, 23.4747],
	[64.8397, 66.0807, 66.2875, 65.8738, 66.2875, 66.0807, 66.0807, 64.4261, 64.0124, 64.8397]]

def box_plot(data, edge_color):
    bp = plt.gca().boxplot(data, patch_artist=True, whis=50)
    
    for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:
        plt.setp(bp[element], color=edge_color, linewidth=0.5)  
        
    for patch in bp['boxes']:
        patch.set(facecolor='white')       

    return bp
    
plt.figure(figsize=(5.5/2,2/2))

# fig, ax = plt.subplots()
bp1 = box_plot(data, 'black')

plt.gca().set_xticklabels(['$x_'+str(i)+'$' for i in range(1,6)])

plt.show()
```

$\quad$

![](figures/trendmining-trend-x3.svg)

![](figures/trendmining-trend-x5.svg)

![](figures/trendmining-space-x3.svg)

![](figures/trendmining-space-x5.svg)

:::
:::
::::

::: {style="font-size: 50%; position: absolute; bottom: -20px;"}
***Paper I***: Smedberg, Henrik, Bandaru, Sunith, Ng, Amos H.C., and Deb, Kalyanmoy (2020). “Trend Mining 2.0: Automating the Discovery of Variable Trends in the Objective Space”. In: *2020 IEEE Congress on Evolutionary Computation (CEC)*, pp. 1–8
:::
:::

::: {.notes} 
1. We develop Trend Mining to find interesting variable trends in the objective space
1. We try to find if the variables follow a monotonic trend along a reference vector
1. answer how changing a variable affects the objective space
1. produces a score, and direction for each variable
:::

## Variable Importance
::: {.slidecontent}

:::: {.columns}
::: {.column width="50%"}
::: {.v-center style="width: 50%"}
* Influence Score
  * Quantify variables' relations to  
    *diversity* or *convergence*
  * InfS-P <small_text>(INFluence Score for *Pareto-front*)</small_text>
  * InfS-R <small_text>(INFluence Score for *Ranks*)<small_text>
  * Finds a *normalized score* for each variable
  * Find expected variable groups in tested problems
  * Applied to an industrial use-case

:::
:::
::: {.column width="50%" style="text-align: center; padding-top: 50px;"}
::: {layout="[[1,1],[1,1],[1,1],[1,1]]" layout-valign="bottom" .tight}

::: {.threequarter}
*InfS-P*
:::

::: {.threequarter}
*InfS-R*
:::

![](figures/infs-dtlz2-3-p.svg)

![](figures/infs-dtlz2-3-r.svg)

![](figures/infs-dtlz2-5-p.svg)

![](figures/infs-dtlz2-5-r.svg)

![](figures/infs-dtlz2-7-p.svg)

![](figures/infs-dtlz2-7-r.svg)

:::
:::
::::

::: {style="font-size: 50%; position: absolute; bottom: -20px;"}
***Paper II***: Smedberg, Henrik and Bandaru, Sunith (2020). “Finding Influential Variables in Multi-Objective optimization Problems”. In: *2020 IEEE Symposium Series on Computational Intelligence (SSCI)*, pp. 173–180
:::
:::

::: {.notes}
I presented influence score as a way to find if variables are related more to diversity or convergence

finds a score for each variable

DTLZ problems have a number of **positional** and **directional** variables that are identified

has also been used in an industrial use-case
:::

## RQ2: *Interactive Decision Support*

::: {.slidecontent}
::: {style="scale: 75%; padding-top: 10px; padding-bottom: 10px;"}
<italic>How can different *representations of knowledge* be processed and *presented* to the decision maker in an *interactive* manner for better *decision support*?</italic>
:::

:::: {.columns}
::: {.column}
* Interactive DSS for knowledge discovery with *FPM rules*
  * Solution exploration
  * Preference elicitation
  * Knowledge discovery
  * Knowledge visualization
:::
::::

::: {style="position: absolute; left: 500px; top: 320px;"}
<span style="font-size: 75%; padding-left: 60px;">FPM rule visualization using graph-based interface</span>

::: {layout="[1,1]" layout-valign="center" style="width:150%;"}
![](figures\ejor-1.png){style="width: 80%"}

![](figures\ejor-2.png){style="width: 40%; margin-left: -100px;"}
:::

:::
::: {style="font-size: 50%; position: absolute; bottom: -20px;"}
***Paper III***: Smedberg, Henrik and Bandaru, Sunith (2023). “Interactive Knowledge Discovery and Knowledge Visualization for Decision Support in Multi-Objective Optimization”. In: *European Journal of Operational Research* 306.3, pp. 1311–1329
:::
:::

::: {.notes}
RQ2 is about interactive knowledge discovery and visualization

In my thesis I present a paper with an interactive DSS framework that enables DM to both discover and visualize knowledge  
With solution exploration for and interaction for preference elicitation,  
and a graph-based visualization of FPM rules
:::

## Mimer: *Openly Available Instantiation of DSS*

::: {.slidecontent}
:::: {.columns}
::: {.column width="50%"}
* Mimer
  * Openly available web tool
  * [https://assar.his.se/mimer/](https://assar.his.se/mimer/html/index.html){target="popup" onclick="window.open('https://assar.his.se/mimer/html/index.html','Mimer','width=1920,height=1080')"}

![](figures\mimer_screenshot.jpg)

:::
::: {.column width="50%"}

* User study
    * Representative real-world tasks
    * 20 participants, overall positive feedback


![](figures\mimerresponses.png){style="z-index: -1; position: absolute; margin-top: -35px; width: 50%;"}

:::
::::

::: {style="font-size: 50%; position: absolute; bottom: -20px;"}
***Paper IV***: Smedberg, Henrik, Bandaru, Sunith, Riveiro, Maria, and Ng, Amos H.C. (2023). “Mimer: A Web-Based Tool for Knowledge Discovery in Multi-Criteria Decision Support”. Submitted to: *IEEE Computational Intelligence Magazine*
:::
:::

::: {.notes}
* This framework was also realized as an openly available web tool --- Mimer
* ...
* Mimer demo
* User study... positive feedback that FPM rules and graph-interface is a useful tool
:::

## Mimer: *Demo*

::: {.slidecontent}
:::: {.columns}
::: {.column}
![](./figures/mimier-gifs/inspect.webm){}
:::
::: {.column .v-center}
* Vehicle crash-worthiness problem
    * Objectives (*minimize*)
        * Total weight
        * Acceleration
        * Toe-board intrusion
    * Variables
        * $x_0$ -- $x_4$
        * Thickness of support pillars
:::
::::
:::

## Mimer: *Demo --- Linked visualizations and interaction*

::: {.slidecontent}
:::: {.columns}
::: {.column}
![](./figures/mimier-gifs/select.webm){}
:::
::: {.column .v-center}
* All opened plots are *linked*
* Lasso selection for *interaction*
* The *DM* can find their *preferred solution*
:::
::::
:::

## Mimer: *Demo --- Knowledge discovery and visualization*

::: {.slidecontent}

::: {.slidecontent}
:::: {.columns}
::: {.column}
![](./figures/mimier-gifs/knowledge.webm){}
:::
::: {.column .v-center}
* *FPM* for knowledge discovery
* Graph-based visualization for filtering

::: {style="height: 40px;"}

:::

* The selection can be entirely described by just one rule 
:::
::::
:::

:::

## RQ3: *Knowledge-Driven Optimization*

::: {.slidecontent}

::: {style="scale: 75%; padding-top: 10px; "}
<italic>**Offline KDO.** How can knowledge extracted from completed optimization runs be utilized *offline* in *future optimizations* of similar problem scenarios to focus the search towards preferred solutions?</italic>
:::


::: {style="scale: 75%; margin-top: -20px; padding-bottom: 10px;"}
<italic>**Online KDO.** How can knowledge extracted during an optimization run be utilized *online* to improve the *convergence behavior* of optimization algorithms?</italic>
:::

:::: {.columns}
::: {.column}
* Offline KDO
    * Discover knowledge from *previous* optimizations
    * Apply this knowledge in *future* optimizations
:::

::: {.column}
*  Online KDO
    * Discover knowledge *during* optimization
    * Apply this knowledge in *current* optimization
:::

::: {style="scale:75%; text-align: center;"}
Both forms of KDO can *inform* the DM
:::
::::
:::

::: {.notes}
rq3 is about KDO and how knowledge can be incorporated in MOEAs

Offline, where the knowledge comes from previous, completed optimizations  
and get applied in future, similar cases

Online, where the knowledge is both generated and applied during an optimization run  
to help the algorithm

Whether generated offline or online, the knowledge itself can still be used to inform the DM
:::



## Offline KDO: *Reconfigurable Manufacturing System Use-case*

::: {.slidecontent}

:::: {.columns}
::: {.column style="padding-top: 20px;"}

::: {style=""}
* <span style="display: block; width: 150%;">Reconfigurable manufacturing system (RMS)</span>
    * Considered *different scenarios* of the same RMS
    * *Grouped scenarios together* based on operators and proportions between parts
    * *Discovered knowledge* among the groups
    * Used *Mimer* for interactive knowledge discovery  

::: {style="height:10px;"}
:::

* Offline KDO
    * Applied knowledge as constraints in the optimization of *new scenarios*
    * Obtained better convergence

:::
:::
::: {.column width="50%" style="padding-top: 80px;"}

::: {style="font-size: 35%;"}

<!-- LTeX: enabled=false -->

| $\begin{gathered} \textbf{Group} \\ \textbf{NO} \;\; \textbf{Proportion} \end{gathered}$ | Rule Interaction | Sig.  | Unsig.  |
| :------------------:                             | :--------------------------------------------:         | :-------: | :-------: |
| 7 <span style="opacity:0;">Proppportion</span>   | $A_{10} = 2 ∧ B_4 \neq 3 ∧ B_5 = 2 ∧ B_{23} = 1$       | 100%      | 10.13%    |
| 8 <span style="opacity:0;">Proppportion</span>   | $A_{17} = 2 ∧ A_{27} \neq 3 ∧ B_7 = 1 ∧ B_{16} = 2$    | 96.43%    | 12.89%    |
| 9 <span style="opacity:0;">Proppportion</span>   | $A_{28} = 1 ∧ B_7 = 1 ∧ B_8 = 3 ∧ B_9 \neq 1$          | 91.67%    | 4.89%     |
| <span style="opacity:0;">NO</span>$\;\;$ 30%/70% | $A_{14} = 3 ∧ B_3 = 1 ∧ B_{10} \neq 1 ∧ B_{23} \neq 3$ | 97.06%    | 29.23%    |
| <span style="opacity:0;">NO</span>$\;\;$ 70%/30% | $A_{14} = 2 ∧ A_{17} = 2 ∧ B_4 \neq 3 ∧ B_{11} \neq 2$ | 92.11%    | 10.89%    |

: {}

<!-- LTeX: enabled=true -->

:::
::: {}
![](figures\offlinekdopaper.png){width="90%" style="z-index: -1; position: relative;"}
:::
:::
::::

::: {style="font-size: 50%; position: absolute; bottom: -20px;"}
***Paper V***: Smedberg, Henrik, Barrera-Diaz, Carlos Alberto, Nourmohammadi, Amir, Bandaru, Sunith, and Ng, Amos H. C. (2022). “Knowledge-Driven Multi-Objective Optimization for Reconfigurable Manufacturing Systems”. In: *Mathematical and Computational Applications 27.6*
:::

:::

::: {.notes}
Industrial cases that validate this workflow  
Different scenarios for an RMS, with different NO and proportions between two parts  
We used Mimer to find this knowledge about task assignment to workstations

offline KDO  
we used this knowledge in optimization of new scenarios  
as hard constraints to apply it as simple as possible  
found better convergence 
:::

## Online KDO: *Knowledge-Driven Preference-Based MOEAs*

::: {.slidecontent}

:::: {.columns}
::: {.column width="50%"}
* *Knowledge-driven* mutation operator
  * Sample an empirical distribution model built from FPM rules
  * Modular approach for preference-based MOEAs
  * R-metric [@li2017r]
  * EH-metric [@ehmetric]

![](figures\onlinekdopaper.png){style="width: 100%; margin-top: -10px; padding-left: 100px;"}

:::
::: {.column width="50%"}

![](figures\kdoflow.drawio.svg){style="width: 85%; margin-left: 50px;"}

:::
::::
::: {style="font-size: 50%; position: absolute; bottom: -20px;"}
***Paper VI***: Smedberg, Henrik and Bandaru, Sunith (2022). “A Modular Knowledge-Driven Mutation Operator for Reference-Point Based Evolutionary Algorithms”. In: *2022 IEEE Congress on Evolutionary Computation (CEC)*, pp. 1–8
:::

:::

::: {.notes}
Online KDO approach following **this** general online KDO MOEA flow  
In this paper we sample an empirical distribution model from FPM rules as new mutation operator  
For preference-based MOEAs using a reference point

We call it *modular* since it can be implemented for any preference-based MOEA   
We compared 3 different ones and found better convergence rates
:::

## KDO Using Variable Importance

::: {.slidecontent}
:::: {.columns}
:::{.column width="50%"}

::: {style="width: 200%;"}
* Variable importance for *Offline* and *Online KDO*
  * Variable importance (*InfS-R*)
  * Discovered either *offline* or *online*
  * Apply knowledge to restrict  
  *convergence-related* variables in crossover
  * Improvement in terms of Hypervolume (HV) and  
    convergence rate (AUC)

![](./figures/infskdo_plots.jpg){style="width:50%;"}
:::

:::
::: {.column width="50%" style="padding-top: 80px;"}

![](figures\infskdopaper.png)

:::
::::

::: {style="font-size: 50%; position: absolute; bottom: -20px;"}
***Paper VII***: Smedberg, Henrik, Bandaru, Sunith, and Ng, Amos H.C (2023). “Knowledge-Driven Multi-Objective Optimization Using Variable Importance”. *Completed Manuscript*
:::

:::

::: {.notes}
This paper looks at the same approach for both offline and online KDO  
We use variable importance (infs-r) to find convergence-related variables 
And try to converge those variables more  
in benchmark problems we can see and improvement (lower is better in the figure)

Offline KDO is the best, online KDO is the second best
:::

# Conclusions

## Conclusions

::: {.slidecontent}
:::: {.columns}
::: {.column}

* Motivation
  * *Knowledge discovery* can benefit optimization and decision making
:::

::: {.column}
* Aim
  * Develop methods for meaningful *knowledge discovery* MOO
  * Appropriate for decision support and KDO
  * *Simple* and easy to understand knowledge 

:::
::::
:::: {.columns .fragment .fade-in}
::: {.column style="font-size: 85%;"}
* *Knowledge Discovery* (RQ1)
  * Appropriate knowledge representations?
    * Trend mining (*Paper I*)
    * Influence score (*Paper II*)
    * Graph-based visualization of FPM-rules (*Paper III*)

* *Interactive Decision Support* (RQ2)
  * How to apply knowledge for decision support?
    * Interactive knowledge discovery (*Paper III*)
    * Mimer (*Paper IV*)
    * RMS use-case (*Paper V*)
:::
::: {.column style="font-size: 85%; margin-top: 50px;"}
* *Knowledge-Driven Optimization* (RQ3)
  * *Offline KDO* (RQ3a)
    * RMS use-case (*Paper V*)
    * Variable importance (*Paper VII*)

  * *Online KDO* (RQ3b)
    * Rule-driven mutation (*Paper VI*)
    * Variable importance (*Paper VII*)
:::
::::
:::


## Future Works

::: {.slidecontent .v-center}
:::: {.columns}

:::: {style="height: 2vw;"}
:::

::: {.column }
* *Knowledge discovery*
  * More knowledge representations
  * Develop clear guidelines for knowledge discovery for practitioners
:::
::: {.column .fragment .fade-in}
* *Decision support*
  * Efficient storing and retrieval of knowledge
  * Group decision making
:::
::::

::: {style="height: 50px"}
:::

:::: {.columns}
::: {.column .fragment .fade-in}
* *Mimer*
  * Add more knowledge discovery methods
  * Add more visualization methods
  * Add compatibility with existing optimization platforms

:::
::: {.column .fragment .fade-in}
* *KDO*
  * Find more appropriate use-cases
  * Algorithms centered around knowledge
  * Knowledge-based metaphors to drive the search
:::
::::
:::

::: {.notes}
::: {style="font-size: 70%;"}
KD: more representations for types of MOOPs, focus on continuous, what about combinatorial?  
Also develop guidelines for practitioners for how knowledge can be discovered and applied

DS: find efficient ways to store the knowledge, and the insights from it, to enable several DMs to work together  
And to find appropriate knowledge for future cases. Also ties together with group decision making

We got a lot of good feedback from the Mimer user study  
And we can add more functionality and plots, and good ideas for the user interface

For online KDO, we applied knowledge discovery as an extra step of the algorithm  
It would be interesting to investigate algorithms centered around knowledge to drive the search  
Instead of a metaphor like *survival of the fittest* (NSGA-II), something like *learning from your mistakes*
:::
:::

## References

<script>
function myFunction() {
  var element = document.getElementById("references");
  element.classList.remove("scrollable");
} 
myFunction();
</script>

::: {#refs style="font-size: 50%; padding-top: 50px;"}
:::

# Thank you


<script>
let videos = document.getElementsByTagName("video");
for (let i = 0; i < videos.length; i++) {
  videos[i].controls = false;
  // videos[i].loop = true;
}
</script>


